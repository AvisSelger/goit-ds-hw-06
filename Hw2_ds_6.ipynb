{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#for the sixth task\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def hypothesis(X, w):\n",
    "    return np.dot(X, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "    m = len(y)\n",
    "    error = hypothesis(X, w) - y\n",
    "    cost = (1 / (2 * m)) * np.dot(error.T, error)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(X, y, w, learning_rate):\n",
    "    m = len(y)\n",
    "    error = hypothesis(X, w) - y\n",
    "    gradient = (1 / m) * np.dot(X.T, error)\n",
    "    w -= learning_rate * gradient\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w, learning_rate, num_iterations):\n",
    "    cost_history = []\n",
    "    for i in range(num_iterations):\n",
    "        w = gradient_descent_step(X, y, w, learning_rate)\n",
    "        cost = compute_cost(X, y, w)\n",
    "        cost_history.append(cost)\n",
    "    return w, cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample data\n",
    "X = np.array([\n",
    "    [2104, 3, 2],\n",
    "    [1600, 3, 2],\n",
    "    [2400, 3, 3],\n",
    "    [1416, 2, 2],\n",
    "    [3000, 4, 3]\n",
    "])\n",
    "y = np.array([400, 330, 369, 232, 540])\n",
    "\n",
    "#adding a unit column to X for a free term\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "#initializing the parameters\n",
    "w = np.zeros(X.shape[1])\n",
    "\n",
    "#model training using gradient descent\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "w_gd, cost_history = gradient_descent(X, y, w, learning_rate, num_iterations)\n",
    "\n",
    "#training a model using an analytics solution\n",
    "w_ne = normal_equation(X, y)\n",
    "\n",
    "#using LinearRegression with sklearn for comparison\n",
    "model = LinearRegression().fit(X[:, 1:], y)\n",
    "w_sklearn = np.r_[model.intercept_, model.coef_]\n",
    "\n",
    "print(\"Parameters from gradient descent:\", w_gd)\n",
    "print(\"Parameters from normal equation:\", w_ne)\n",
    "print(\"Parameters from sklearn LinearRegression:\", w_sklearn)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
